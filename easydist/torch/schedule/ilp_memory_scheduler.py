from collections import defaultdict, OrderedDict

import sys
import logging
import time
import torch
import intervaltree

import easydist.config as mdconfig
from easydist.torch.schedule.lifetime_info import LifetimeInfo, OverlapType
from easydist.torch.schedule.memory_scheduler import MemoryScheduler

from mip import (
    Model,
    xsum,
    BINARY,
    INTEGER,
    minimize,
    SearchEmphasis,
    OptimizationStatus,
)

logger = logging.getLogger(__name__)

class ILPMemoryScheduler(MemoryScheduler):
    """
    ILP Memory scheduler is inspired by [MODel](https://proceedings.mlr.press/v202/steiner23a/steiner23a.pdf).
    We build the python-mip model on fx graph IR. A customized memory allocator
    will be created which follow the memory allocation plan generated by memory
    scheduler.
    """
    def __init__(
        self,
        fx_module,      # torch.fx.GraphModule
        graph_mem_info, # GraphMemInfo
        align_scale=4,
        timeout_s=None,
        rel_stop=0.01,
        abs_stop=8,
        one_step_one_op=True
    ):
        super().__init__(fx_module, graph_mem_info, align_scale)

        self.timeout = timeout_s
        self.rel_stop = rel_stop
        self.abs_stop = abs_stop
        self.one_step_one_op = one_step_one_op

        # Lansong(TODO):
        # refresh nodes_to_schedule and node_set by dropping out placeholder
        # It is a workaround because currently placeholder memory information
        # is missed. Once placeholder memory information is fixed, the refresh
        # should be removed.
        self.nodes_to_schedule = []
        for node in fx_module.graph.nodes:
            if node.op == 'placeholder' or node.op == 'get_attr':
                self.args.append(node)
            elif node.op == 'output':
                self.outputs.append(node)
            else:
                self.nodes_to_schedule.append(node)

        self.node_set = set(self.nodes_to_schedule)

    def total_tensor_size(self):
        total_size = 0
        total_align_scaled_size = 0
        for node in self.nodes_to_schedule:
            out_vars = self.graph_mem_info.get_out_vars(node)
            for var in out_vars:
                re_num = var.size()%self.align_scale
                if re_num > 0:
                    align_size = var.size()+self.align_scale-re_num
                else:
                    align_size = var.size()
                total_size += align_size
                total_align_scaled_size += (align_size//self.align_scale)

        return (total_size, total_align_scaled_size)

    def build_ilp_model(
        self,
        mem_limit=sys.maxsize,
        pre_scheded_nodes=None,
    ):

        lifetime_info = LifetimeInfo(self.nodes_to_schedule,
                                     self.graph_mem_info,
                                     pre_scheded_nodes,
                                     self.one_step_one_op)
        edge_makespans = lifetime_info.compute_edge_makespans()
        print("asap and alap information:")
        print(lifetime_info.node_makespans)

        self.num_steps = lifetime_info.num_steps

        print(edge_makespans)

        total_size, total_align_scaled_size = self.total_tensor_size()
        print("total_size: %d, total_align_scaled_size: %d" % (total_size, total_align_scaled_size))
        max_address = (min(total_size, mem_limit)+self.gcd-1) // self.gcd
        print("max_address: %d" % max_address)

        int_feas_tol = min(1e-5, 1.0 / max_address)
        int_feas_tol = max(1e-9, int_feas_tol)

        model = Model("mem_scheduler")
        model.max_mip_gap = self.rel_stop
        model.max_mip_gap_abs = self.abs_stop
        model.integer_tol = int_feas_tol              # set integer_tol
        model.emphasis = SearchEmphasis.FEASIBILITY   # mean "MIPFocus": 1
        #model.emphasis = SearchEmphasis.OPTIMALITY    # mean "MIPFocus": 2   # Lansong(TODO)
        model.threads = 64

        # Create two variables for each tensor and timestep:
        create_vars = defaultdict(lambda: defaultdict(lambda: {}))
        preserve_vars = defaultdict(lambda: defaultdict(lambda: {}))

        create_vars_per_step = [[] for _ in range(self.num_steps)]
        for node in self.nodes_to_schedule:
            lb, ub = edge_makespans[node]
            assert lb < self.num_steps, f"node({node.name}): lb({lb}) is not less than num_steps({self.num_steps})"
            assert ub < self.num_steps, f"node({node.name}): ub({ub}) is not less than num_steps({self.num_steps})"
            #print(("node name: {}, node op: {}, span: ({},{})").format(node,node.op,lb,ub))
            out_vars = self.graph_mem_info.get_out_vars(node)
            #print(("out_vars: {}").format(out_vars))
            for step in range(lb, ub+1):
                for idx,out_var in enumerate(out_vars):
                    out_idx = out_var.out_index
                    out_name = node.name + "_" + str(out_idx) + "_"
                    out_name = out_name.replace("[", "_")
                    out_name = out_name.replace("]", "_")

                    var_name = out_name + "_create_step" + str(step)
                    v = model.add_var(var_type=BINARY, name=var_name)
                    #print(("create/preserve: node: {}, out_idx: {}, step: {}").format(node, out_idx, step))
                    create_vars[node][out_idx][step] = v
                    if self.one_step_one_op and (idx == 0):
                        create_vars_per_step[step].append(v)

                    var_name = out_name + "_preserve_step" + str(step)
                    v = model.add_var(var_type=BINARY, name=var_name)
                    preserve_vars[node][out_idx][step] = v

            if pre_scheded_nodes:
                if node in pre_scheded_nodes:
                    # node is scheduled by user
                    scheded_step = pre_scheded_nodes[node]
                    assert scheded_step >= lb
                    assert scheded_step <= ub
                    for step in range(lb, ub+1):
                        if step != scheded_step:
                            for out_var in out_vars:
                                out_idx = out_var.out_index
                                model += create_vars[node][out_idx][step] == 0
                        else:
                            for out_var in out_vars:
                                out_idx = out_var.out_index
                                model += create_vars[node][out_idx][step] == 1
                for snk in node.users:
                    if snk in pre_scheded_nodes:
                        snk_step = pre_scheded_nodes[snk]
                        assert snk_step >= lb
                        assert snk_step <= ub
                        for out_var in out_vars:
                            out_idx = out_var.out_index
                            model += preserve_vars[node][out_idx][snk_step] == 1

        # 1. One and only one node is scheduled to each step
        if self.one_step_one_op:
            for one_step_vars in create_vars_per_step:
                model += xsum(var for var in one_step_vars) == 1

        # 2. A tensor can either be created or preserved at a step, but not both
        for node in self.nodes_to_schedule:
            lb, ub = edge_makespans[node]
            for step in range(lb, ub+1):
                out_vars = self.graph_mem_info.get_out_vars(node)
                for out_var in out_vars:
                    out_idx = out_var.out_index
                    model += preserve_vars[node][out_idx][step] + \
                             create_vars[node][out_idx][step] <= 1

        # 3. A tensor can be preserved at a step if and only if it was created or
        #    preserved at the previous step
        for node in self.nodes_to_schedule:
            lb, ub = edge_makespans[node]
            for step in range(lb+1, ub+1):
                out_vars = self.graph_mem_info.get_out_vars(node)
                for out_var in out_vars:
                    out_idx = out_var.out_index
                    model += preserve_vars[node][out_idx][step] <= \
                             preserve_vars[node][out_idx][step-1] + \
                             create_vars[node][out_idx][step-1]

        # 4. Force every tensor to be created once
        for node in self.nodes_to_schedule:
            lb, ub = edge_makespans[node]
            out_vars = self.graph_mem_info.get_out_vars(node)
            for out_var in out_vars:
                out_idx = out_var.out_index
                model += xsum(create_vars[node][out_idx][step] for step in range(lb, ub+1)) == 1

        # 5. all inputs must be present in memory when evaluating a node
        for node in self.nodes_to_schedule:
            node_lb, node_ub = lifetime_info.node_makespans[node]
            for arg in node.args:
                if isinstance(arg, torch.fx.Node):
                    if arg not in self.node_set:
                        continue
                    for step in range(node_lb, node_ub+1):
                        #print(("node: {}, arg: {}, step: {}").format(node, arg, step))
                        model += create_vars[node][0][step] <= \
                                 preserve_vars[arg][0][step]

        # 6. all outputs of a node must be created at the same time
        for node in self.nodes_to_schedule:
            out_vars = self.graph_mem_info.get_out_vars(node)
            if len(out_vars) > 1:
                node_lb, node_ub = lifetime_info.node_makespans[node]
                for var_idx in range(1, len(out_vars)):
                    out_idx = out_vars[var_idx].out_index
                    for step in range(node_lb, node_ub+1):
                        model += create_vars[node][0][step] == \
                                 create_vars[node][out_idx][step]

        # Auxiliary constraints which are not MUST but helpful to speedup solver
        #   Auxiliary constraint 1:
        for node in self.nodes_to_schedule:
            lb, _ = edge_makespans[node]
            out_vars = self.graph_mem_info.get_out_vars(node)
            for out_var in out_vars:
                out_idx = out_var.out_index
                model += preserve_vars[node][out_idx][lb] == 0

        #   Auxiliary constraint 2:
        for node in self.nodes_to_schedule:
            lb, ub = edge_makespans[node]
            _, src_node_alap = lifetime_info.node_makespans[node]
            out_vars = self.graph_mem_info.get_out_vars(node)
            for out_var in out_vars:
                out_idx = out_var.out_index
                for step in range(lb, ub+1):
                    if step > src_node_alap:
                        model += create_vars[node][out_idx][step] == 0

        # Do not need the constraint that all inputs are alive.
        # because following is always true:
        # at least one timestep during which all the inputs are live at the same time

        #   Auxiliary constraint 3:
        for node in self.nodes_to_schedule:
            _, node_ub = lifetime_info.node_makespans[node]
            latest_preserve_step = node_ub + 1
            last_read_step = 0
            for user in node.users:
                if user not in self.node_set:
                    continue
                user_lb, _ = lifetime_info.node_makespans[user]
                last_read_step = max(last_read_step, user_lb)
            if latest_preserve_step > last_read_step:
                continue
            edge_lb, edge_ub = edge_makespans[node]
            out_vars = self.graph_mem_info.get_out_vars(node)
            for out_var in out_vars:
                out_idx = out_var.out_index
                for step in range(edge_lb, edge_ub+1):
                    if step < latest_preserve_step or step > last_read_step:
                        continue
                    model += preserve_vars[node][out_idx][step] == 1

        # memory usage at each step
        mem_per_step = defaultdict(lambda: 0)
        for step in range(self.num_steps):
            for node in self.nodes_to_schedule:
                out_vars = self.graph_mem_info.get_out_vars(node)
                for out_var in out_vars:
                    out_idx = out_var.out_index
                    out_size = out_var.size()
                    create_step_map = create_vars[node][out_idx]
                    if step in create_step_map.keys():
                        mem_per_step[step] = mem_per_step[step] + create_step_map[step] * ((out_size+self.gcd-1) // self.gcd)
                    preserve_step_map = preserve_vars[node][out_idx]
                    if step in preserve_step_map.keys():
                        mem_per_step[step] = mem_per_step[step] + preserve_step_map[step] * ((out_size+self.gcd-1) // self.gcd)

        #   Auxiliary constraint 4:
        # memory limit constraints
        liveness = defaultdict(lambda: [])
        for node in self.nodes_to_schedule:
            edge_lb, edge_ub = edge_makespans[node]
            out_vars = self.graph_mem_info.get_out_vars(node)
            for out_var in out_vars:
                for step in range(edge_lb, edge_ub+1):
                    liveness[step].append(out_var)

        for step, mem_usage in mem_per_step.items():
            max_mem = 0
            for var in liveness[step]:
                max_mem += var.size()
            if max_mem < mem_limit:
                continue
            model += mem_usage <= (mem_limit+self.gcd-1) // self.gcd

        # generate addresses
        gen_allocation = False
        manual_alloc = True
        if pre_scheded_nodes:
            gen_allocation = True
            for node in self.nodes_to_schedule:
                if node not in pre_scheded_nodes:
                    manual_alloc = False
                    break

        addresses = OrderedDict()
        if gen_allocation:
            # encode tensor locations
            for node in self.nodes_to_schedule:
                out_vars = self.graph_mem_info.get_out_vars(node)
                node_out_addr_vars = [None]*len(out_vars)
                for out_var in out_vars:
                    out_idx = out_var.out_index
                    out_size = out_var.size()
                    assert out_idx < len(out_vars)
                    assert out_size > 0
                    var_name = node.name.replace("[", "_")
                    var_name = var_name.replace("]", "_")
                    addr_var = model.add_var(var_type=INTEGER,
                                             name="%s_o_%d" % (var_name, out_idx),
                                             lb=0,
                                             ub=max_address-((out_size+self.gcd-1)//self.gcd))
                    node_out_addr_vars[out_idx] = addr_var
                addresses[node] = node_out_addr_vars

            fixed_locations = {}
            if manual_alloc:
                min_start = 0
                max_end = self.num_steps
                base_address = 0
                processed_nodes = set()
                mem_used = intervaltree.IntervalTree()
                while max_end > min_start:
                    max_duration = 0
                    next_tensors = None
                    next_nd = None

                    for nd, span in edge_makespans.items():
                        if span[0] < min_start:
                            continue
                        if span[1] > max_end:
                            continue
                        if nd in processed_nodes:
                            continue
                        out_vars = self.graph_mem_info.get_out_vars(nd)
                        candidate_tensors = []
                        for var in out_vars:
                            if var.is_reference:
                                continue
                            candidate_tensors.append(var)

                        if not candidate_tensors:
                            continue

                        duration = span[1] - span[0]
                        if duration > max_duration:
                            max_duration = duration
                            next_tensors = candidate_tensors
                            next_nd = nd
                    if not next_nd:
                        break

                    min_start = edge_makespans[next_nd][0]
                    max_end = edge_makespans[next_nd][1]
                    processed_nodes.add(next_nd)
                    for var in next_tensors:
                        model += addresses[next_nd][var.out_index] == base_address
                        fixed_locations[(next_nd, var.out_index)] = base_address
                        base_address += (var.mem_size+self.gcd-1)//self.gcd

                    mem_used[min_start : max_end + 1] = base_address

                print(("mem used: {}").format(mem_used))
                print("base_address: %d" % base_address)
                max_mem = base_address
                for nd, addr_lst in addresses.items():
                    out_vars = self.graph_mem_info.get_out_vars(nd)
                    assert len(out_vars) == len(addr_lst)
                    for out_var in out_vars:
                        if out_var.is_reference:
                            continue
                        if (nd, out_var.out_index) not in fixed_locations:
                            span = edge_makespans[nd]
                            max_address_used = 0
                            print(("mem used overlap: {} with span {}").format(mem_used.overlap(span[0], span[1] + 1), span))
                            for interval in mem_used.overlap(span[0], span[1] + 1):
                                print(f"address {interval.data} is used, not avaliable for span [{span}]")
                                max_address_used = max(max_address_used, interval.data)
                            addr_var = addresses[nd][out_var.out_index]
                            addr_var.lb = max_address_used

                            new_address = max_address_used + (out_var.mem_size+self.gcd-1)//self.gcd
                            print(("node {}: out idx: {}, lb: {}, ub: {}\n").format(nd, out_var.out_index, max_address_used, new_address))

                            mem_used[span[0] : span[1] + 1] = new_address
                            max_mem = max(max_mem, new_address)

                print("max mem: %d" % max_mem)
                for nd, addr_lst in addresses.items():
                    out_vars = self.graph_mem_info.get_out_vars(nd)
                    for out_var in out_vars:
                        if out_var.is_reference:
                            continue
                        if (nd, out_var.out_index) not in fixed_locations:
                            addr = addr_lst[out_var.out_index]
                            model += addr <= max_mem - (out_var.mem_size+self.gcd-1)//self.gcd

            processed_node_pairs = set()
            for nd1, span1 in edge_makespans.items():
                for nd2, span2 in edge_makespans.items():
                    if nd1 is nd2:
                        continue
                    if (nd2, nd1) in processed_node_pairs:
                        continue
                    processed_node_pairs.add((nd1, nd2))
                    overlap_live_start = max(span1[0], span2[0])
                    overlap_live_stop = min(span1[1], span2[1])

                    assert ((span1[1] < span2[0] or span1[0] > span2[1]) ==
                           (overlap_live_stop < overlap_live_start))
                    out_vars1 = self.graph_mem_info.get_out_vars(nd1)
                    out_vars2 = self.graph_mem_info.get_out_vars(nd2)
                    for var1 in out_vars1:
                        idx1 = var1.out_index
                        for var2 in out_vars2:
                            idx2 = var2.out_index
                            overlap_type = lifetime_info.get_overlap_type(
                                                              nd1, idx1, nd2, idx2)
                            if overlap_type == OverlapType.NEVER_OVERLAP:
                                # need not add constraint for two memory addresses
                                continue
                            elif overlap_type == OverlapType.FORCE_SHARE:
                                model += addresses[nd1][idx1] == addresses[nd2][idx2]
                            elif overlap_type == OverlapType.OVERLAP_NOT_SHARE:
                                # create a binary var to represent var1 memory is
                                # below var2 memory
                                v_name = nd1.name + "_" + str(idx1) + "_below_" + \
                                         nd2.name + "_" + str(idx2)
                                v_name = v_name.replace("[", "_")
                                v_name = v_name.replace("]", "_")
                                v1_below_v2 = model.add_var(var_type=BINARY, name=v_name)
                                model += addresses[nd1][idx1] + ((var1.size()+self.gcd-1)//self.gcd) \
                                         - addresses[nd2][idx2] <= \
                                         (1-v1_below_v2)*max_address
                                model += addresses[nd2][idx2] + ((var2.size()+self.gcd-1)//self.gcd) \
                                         - addresses[nd1][idx1] <= \
                                         v1_below_v2*max_address
                            else:
                                assert overlap_type == OverlapType.POSSIBLE_OVERLAP
                                # It is possible that two tensor's memory using
                                # spans overlap.
                                v_base_name = nd1.name + "_" + str(idx1) + "_" + \
                                              nd2.name + "_" + str(idx2) + "_"
                                v_base_name = v_base_name.replace("[", "_")
                                v_base_name = v_base_name.replace("]", "_")
                                v1_name = v_base_name + "_v1"
                                v1 = model.add_var(var_type=BINARY, name=v1_name)
                                v2_name = v_base_name + "_v2"
                                v2 = model.add_var(var_type=BINARY, name=v2_name)
                                model += v1 + v2 <= 1

                                for step in range(overlap_live_start, overlap_live_stop+1):
                                    live1 = create_vars[nd1][idx1][step] + \
                                            preserve_vars[nd1][idx1][step]
                                    live2 = create_vars[nd2][idx2][step] + \
                                            preserve_vars[nd2][idx2][step]
                                    overlap_at_step = live1 + live2 - 1
                                    model += v1 + v2 >= overlap_at_step

                                model += addresses[nd1][idx1] + ((var1.size()+self.gcd-1)//self.gcd) - \
                                         addresses[nd2][idx2] <= (1-v1)*max_address
                                model += addresses[nd1][idx1] - addresses[nd2][idx2] \
                                         - ((var2.size()+self.gcd-1)//self.gcd) >= (v2-1)*max_address

        # construct optimization objective
        min_peak_mem = 0  # Lansong(TODO)
        peak_mem_usage = model.add_var(var_type=INTEGER, name="peak_mem_usage",
                                       lb=min_peak_mem, ub=max_address)

        if addresses:
            # constraint for peak memory usage
            for nd, addr_lst in addresses.items():
                out_vars = self.graph_mem_info.get_out_vars(nd)
                assert len(out_vars) == len(addr_lst)
                for out_var in out_vars:
                    out_idx = out_var.out_index
                    out_size = out_var.size()
                    addr = addr_lst[out_idx]
                    print(("addr var: {}, aligned size: {}").format(addr, ((out_size+self.gcd-1)//self.gcd)))
                    model += peak_mem_usage >= addr + ((out_size+self.gcd-1)//self.gcd)

        for step, mem_usage in mem_per_step.items():
            model += peak_mem_usage >= mem_usage

        obj = peak_mem_usage

        # set optimization objective
        model.verbose = 1
        model.objective = minimize(obj)

        return model, addresses, create_vars, preserve_vars, peak_mem_usage

    def create_min_mem_plan(self, pre_scheded_nodes=None):
        model, addresses, create_vars, preserve_vars, peak_mem_usage = \
                       self.build_ilp_model(pre_scheded_nodes=pre_scheded_nodes)

        model.write("./tmp/addr_gen.lp")

        start_time = time.time()
        logger.info("Start ILP solver for minimize memory usage")
        if self.timeout:
            model.optimize(max_seconds=self.timeout)
        else:
            model.optimize()
        logger.info(f"ILP solver time: {time.time()-start_time} seconds")

        print(("model.status: {}").format(model.status))
        print(("model.num_solutions: {}").format(model.num_solutions))
        print(("model.objective_value: {}").format(model.objective_value))
        if model.num_solutions:
            if model.status == OptimizationStatus.OPTIMAL:
                logger.info(f"Optimal solution was found with objective value: {model.objective_value}")
            else:
                assert model.status == OptimizationStatus.FEASIBLE
                logger.info(f"Feasible solution was found with objective value: {model.objective_value}")
        else:
            logger.info("No solution was found")

        if (
            model.status == OptimizationStatus.INFEASIBLE
            or model.status == OptimizationStatus.NO_SOLUTION_FOUND
        ):
            return (None, None, None, None)

        # extract schedules for return
        schedules = defaultdict(lambda: 0)
        for node in self.nodes_to_schedule:
            if node not in self.node_set:
                continue
            nd_out_create_step = -1
            print(("type of create_vars[node][0]: {}").format(type(create_vars[node][0])))
            print(("len of create_vars[node][0]: {}").format(len(create_vars[node][0])))
            for step,val in create_vars[node][0].items():
                print(("step: {}, val: {}, val.x: {}").format(step, val, val.x))
                if val.x >= 0.99:
                    nd_out_create_step = step
                    break
            assert nd_out_create_step >= 0
            schedules[node] = nd_out_create_step

        steped_nodes = [[] for _ in range(self.num_steps)]
        for nd,step in schedules.items():
            assert step < self.num_steps
            steped_nodes[step].append(nd)

        ordered_schedules = []
        for step, nodes_in_step in enumerate(steped_nodes):
            print("step: %d" % step)
            for nd in nodes_in_step:
                print(("  node: {}").format(nd))
                ordered_schedules.append(nd)
        assert len(ordered_schedules) == len(self.nodes_to_schedule)

        mem_locations = defaultdict(lambda: [])
        for node,addrs in addresses.items():
            mem_addrs = []
            out_vars = self.graph_mem_info.get_out_vars(node)
            for idx,addr in enumerate(addrs):
                mem_size = 0
                if out_vars[idx].out_index == idx:
                    mem_size = out_vars[idx].size()
                else:
                    # in case out_vars are out of order
                    for out_var in out_vars:
                        if out_var.out_index == idx:
                            mem_size = out_var.size()
                            break
                assert mem_size > 0
                logger.info(f"{node}:{out_vars[idx].out_index}: addr: {addr.x}-{addr.x-1+(mem_size+self.gcd-1)//self.gcd}, orig size: {mem_size}")
                mem_addr = int(addr.x + 0.5)*self.gcd
                mem_addrs.append((mem_addr, mem_size))
            mem_locations[node] = mem_addrs

        required_memory = int(peak_mem_usage.x + 0.5)*self.gcd

        # dump peak memory
        logger.info(f"required memory: {required_memory/1024/1024/1024}GB")

        # dump memory addresses
        graph_mem_addr_str = "graph memory addresses:\n"
        for node,mem_addrs in mem_locations.items():
            node_mem_str = node.name + ": "
            for mem_addr_size in mem_addrs:
                node_mem_str += "([" + str(mem_addr_size[0]) + "~" + \
                                str(mem_addr_size[0]+mem_addr_size[1]-1) + "], " + \
                                str(mem_addr_size[1]) + "), "

            graph_mem_addr_str += node_mem_str + "\n"
        logger.info(graph_mem_addr_str)

        # dump ordered schedules
        ordered_nodes_str = "ordered nodes:\n"
        for idx,nd in enumerate(ordered_schedules):
            assert nd
            ordered_nodes_str += str(idx) + ": " + nd.name + "\n"
        logger.info(ordered_nodes_str)

        return (required_memory, schedules, ordered_schedules, mem_locations)

    # Lansong(TODO): override implementation of base class
    # The difference in this function is that it drop out placeholder.
    # It is a workaround because currently placeholder memory information
    # is missed. Once placeholder memory information is fixed, following
    # function should be removed and reuse the implementation of base class.
    def gen_mem_addresses(self):
        pre_scheded_nodes = None
        if not mdconfig.enable_reschedule:
            pre_scheded_nodes = {}
            step = 0
            for node in self.fx_module.graph.nodes:
                if (
                    node.op != 'output'
                    and node.op != 'placeholder'
                    and node.op != 'get_attr'
                ):
                    pre_scheded_nodes[node] = step
                    step += 1

        required_memory, schedules, ordered_schedules, mem_locations = \
                                      self.create_min_mem_plan(
                                          pre_scheded_nodes=pre_scheded_nodes)

        return (required_memory, schedules, ordered_schedules, mem_locations)

